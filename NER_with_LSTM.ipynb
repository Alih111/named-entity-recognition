{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alih111/named-entity-recognition/blob/main/NER_with_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6czvz5VKO5M"
      },
      "source": [
        "# Notebook for Programming in Problem 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5o8HI5JqTvU5"
      },
      "source": [
        "## Learning Objectives\n",
        "In this problem, we will use [PyTorch](https://pytorch.org/) to implement long short-term memory (LSTM) for named entity recognition (NER). We will use the same dataset and boilerplate code as in Programming Problem 1 of Assignment #3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObrHyvWvTyGZ"
      },
      "source": [
        "## Writing Code\n",
        "Look for the keyword \"TODO\" and fill in your code in the empty space.\n",
        "Feel free to change function signatures, but be careful that you might need to also change how they are called in other parts of the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r6YTnpgbFdMI",
        "outputId": "14768faf-c702-4ca1-c70b-bb1eb3f77a32",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Apr 29 11:06:38 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8              10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi # you may need to try reconnecting to get a T4 gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnYMKJlKNXYe"
      },
      "source": [
        "## Installing PyTorch and Other Packages\n",
        "\n",
        "Install PyTorch using pip. See [https://pytorch.org/](https://pytorch.org/) if you want to install it on your computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dRVuiP_JVdT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0e60cb3-7455-402e-bcc7-dd56259039a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.10/dist-packages (0.17.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torchtext) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchtext) (2.31.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchtext) (1.25.2)\n",
            "Requirement already satisfied: torchdata==0.7.1 in /usr/local/lib/python3.10/dist-packages (from torchtext) (0.7.1)\n",
            "Requirement already satisfied: urllib3>=1.25 in /usr/local/lib/python3.10/dist-packages (from torchdata==0.7.1->torchtext) (2.0.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (3.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchtext) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchtext -f https://download.pytorch.org/whl/torch_stable.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPsFH637OpLy"
      },
      "source": [
        "Test if our installation works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c62StNb2NvKk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeec57bb-7d0e-4d4b-fb2b-266775d1e8dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch successfully installed!\n",
            "Version: 2.2.1+cu121\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Multiply two matrices on GPU\n",
        "a = torch.rand(100, 200).cuda()\n",
        "b = torch.rand(200, 100).cuda()\n",
        "c = torch.matmul(a, b)\n",
        "\n",
        "print(\"PyTorch successfully installed!\")\n",
        "print(\"Version:\", torch.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qaC8sxcqkGX"
      },
      "source": [
        "Also install [scikit-learn](https://scikit-learn.org/stable/). We will use it for calculating evaluation metrics such as accuracy and F1 score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5Y2xB_uqqM9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b36715d6-ddb7-45ea-df6c-17651d6676d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.4.0)\n",
            "Installing collected packages: scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "Successfully installed scikit-learn-1.4.2\n"
          ]
        }
      ],
      "source": [
        "!pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhV4CYivRbt4"
      },
      "source": [
        "Let's import all the packages at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjRM4cCFRh-d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchtext.vocab import Vocab, vocab\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
        "import re\n",
        "from collections import Counter\n",
        "from typing import List, Tuple, Dict, Optional, Any"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yn1bIPjAN-9V"
      },
      "source": [
        "## Long Short Term Memory (LSTM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJOKIneRTrTH"
      },
      "source": [
        "### Data Loading\n",
        "\n",
        "We will use the same dataset for named entity recognition in Assignment #2. First download the data and take a look at the first 50 lines:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWqz7kDxSqeb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a49d8d8a-35a7-4e36-a0e0-6c36312e0327"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EU NNP I-NP ORG\n",
            "rejects VBZ I-VP O\n",
            "German JJ I-NP MISC\n",
            "call NN I-NP O\n",
            "to TO I-VP O\n",
            "boycott VB I-VP O\n",
            "British JJ I-NP MISC\n",
            "lamb NN I-NP O\n",
            ". . O O\n",
            "\n",
            "Peter NNP I-NP PER\n",
            "Blackburn NNP I-NP PER\n",
            "\n",
            "BRUSSELS NNP I-NP LOC\n",
            "1996-08-22 CD I-NP O\n",
            "\n",
            "The DT I-NP O\n",
            "European NNP I-NP ORG\n",
            "Commission NNP I-NP ORG\n",
            "said VBD I-VP O\n",
            "on IN I-PP O\n",
            "Thursday NNP I-NP O\n",
            "it PRP B-NP O\n",
            "disagreed VBD I-VP O\n",
            "with IN I-PP O\n",
            "German JJ I-NP MISC\n",
            "advice NN I-NP O\n",
            "to TO I-PP O\n",
            "consumers NNS I-NP O\n",
            "to TO I-VP O\n",
            "shun VB I-VP O\n",
            "British JJ I-NP MISC\n",
            "lamb NN I-NP O\n",
            "until IN I-SBAR O\n",
            "scientists NNS I-NP O\n",
            "determine VBP I-VP O\n",
            "whether IN I-SBAR O\n",
            "mad JJ I-NP O\n",
            "cow NN I-NP O\n",
            "disease NN I-NP O\n",
            "can MD I-VP O\n",
            "be VB I-VP O\n",
            "transmitted VBN I-VP O\n",
            "to TO I-PP O\n",
            "sheep NN I-NP O\n",
            ". . O O\n",
            "\n",
            "Germany NNP I-NP LOC\n",
            "'s POS B-NP O\n",
            "representative NN I-NP O\n"
          ]
        }
      ],
      "source": [
        "!wget --quiet https://princeton-nlp.github.io/cos484/assignments/a2/eng.train\n",
        "!wget --quiet https://princeton-nlp.github.io/cos484/assignments/a2/eng.val\n",
        "!cat eng.train | head -n 50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVt1a6nzWsiF"
      },
      "source": [
        "Each line corresponds to a word. Different sentences are separated by an additional line break. Take \"EU NNP I-NP ORG\" as an example. \"EU\" is a word. \"NNP\" and \"I-NP\" are tags for POS tagging and chunking, which we will ignore. \"ORG\" is the tag for NER, which is our prediction target. There are 5 possible values for the NER tag: ORG, PER, LOC, MISC, and O.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnNfOBUYJvVW"
      },
      "outputs": [],
      "source": [
        "# A sentence is a list of (word, tag) tuples.\n",
        "# For example, [(\"hello\", \"O\"), (\"world\", \"O\"), (\"!\", \"O\")]\n",
        "Sentence = List[Tuple[str, str]]\n",
        "\n",
        "\n",
        "def read_data_file(\n",
        "    datapath: str,\n",
        ") -> Tuple[List[Sentence], Dict[str, int], Dict[str, int]]:\n",
        "    \"\"\"\n",
        "    Read and preprocess input data from the file `datapath`.\n",
        "    Example:\n",
        "    ```\n",
        "        sentences, word_cnt, tag_cnt = read_data_file(\"eng.train\")\n",
        "    ```\n",
        "    Return values:\n",
        "        `sentences`: a list of sentences, including words and NER tags\n",
        "        `word_cnt`: a Counter object, the number of occurrences of each word\n",
        "        `tag_cnt`: a Counter object, the number of occurences of each NER tag\n",
        "    \"\"\"\n",
        "    sentences: List[Sentence] = []\n",
        "    word_cnt: Dict[str, int] = Counter()\n",
        "    tag_cnt: Dict[str, int] = Counter()\n",
        "\n",
        "    for sentence_txt in open(datapath).read().split(\"\\n\\n\"):\n",
        "        if \"DOCSTART\" in sentence_txt:\n",
        "            # Ignore dummy sentences at the begining of each document.\n",
        "            continue\n",
        "        # Read a new sentence\n",
        "        sentences.append([])\n",
        "        for token in sentence_txt.split(\"\\n\"):\n",
        "            w, _, _, t = token.split()\n",
        "            # Replace all digits with \"0\" to reduce out-of-vocabulary words\n",
        "            w = re.sub(\"\\d\", \"0\", w)\n",
        "            word_cnt[w] += 1\n",
        "            tag_cnt[t] += 1\n",
        "            sentences[-1].append((w, t))\n",
        "\n",
        "    return sentences, word_cnt, tag_cnt\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WLMGYSZ7KxzP"
      },
      "outputs": [],
      "source": [
        "# Some helper code\n",
        "def get_device() -> torch.device:\n",
        "    \"\"\"\n",
        "    Use GPU when it is available; use CPU otherwise.\n",
        "    See https://pytorch.org/docs/stable/notes/cuda.html#device-agnostic-code\n",
        "    \"\"\"\n",
        "    return torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wVHAOb7iMPwC"
      },
      "outputs": [],
      "source": [
        "def eval_metrics(ground_truth: List[int], predictions: List[int]) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Calculate various evaluation metrics such as accuracy and F1 score\n",
        "    Parameters:\n",
        "        `ground_truth`: the list of ground truth NER tags\n",
        "        `predictions`: the list of predicted NER tags\n",
        "    \"\"\"\n",
        "    f1_scores = f1_score(ground_truth, predictions, average=None)\n",
        "    return {\n",
        "        \"accuracy\": accuracy_score(ground_truth, predictions),\n",
        "        \"f1\": f1_scores,\n",
        "        \"average f1\": np.mean(f1_scores),\n",
        "        \"confusion matrix\": confusion_matrix(ground_truth, predictions),\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s830dhbnj1L"
      },
      "source": [
        "## Long Short-term Memory (LSTM)\n",
        "\n",
        "Now we implement an one-layer LSTM for the same task and compare it to FFNNs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "to7DnWNiY5ZS"
      },
      "source": [
        "### Data Loading **(4 points)**\n",
        "\n",
        "Like before, we first implement the data loader. But unlike before, each data example is now a variable-length sentence. How can we pack multiple sentences with different lengths into the same batch? One possible solution is to pad them to the same length using a special token. The code below illustrates the idea:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5oVgqE7JaJp"
      },
      "outputs": [],
      "source": [
        "# 3 sentences with different lengths\n",
        "sentence_1 = torch.tensor([6, 1, 2])\n",
        "sentence_2 = torch.tensor([4, 2, 7, 7, 9])\n",
        "sentence_3 = torch.tensor([3, 4])\n",
        "# Form a batch by padding 0\n",
        "sentence_batch = torch.tensor([\n",
        "    [6, 1, 2, 0, 0],\n",
        "    [4, 2, 7, 7, 9],\n",
        "    [3, 4, 0, 0, 0],\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udC0SMjkKaCN"
      },
      "source": [
        "We implement the above idea in a customized batching function `form_batch`. Optionally, see [here](https://pytorch.org/docs/stable/data.html#loading-batched-and-non-batched-data) for how batching works in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sACcGN4XYMgj"
      },
      "outputs": [],
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each data example is a sentence, including its words and NER tags.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, datapath: str, words_vocab: Optional[Vocab] = None, tags_vocab: Optional[Vocab] = None\n",
        "    ) -> None:\n",
        "        \"\"\"\n",
        "        Initialize the dataset by reading from datapath.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.sentences: List[Sentence] = []\n",
        "        UNKNOWN = \"<UNKNOWN>\"\n",
        "        PAD = \"<PAD>\"  # Special token used for padding\n",
        "\n",
        "        print(\"Loading data from %s\" % datapath)\n",
        "        self.sentences, word_cnt, tag_cnt = read_data_file(datapath)\n",
        "        print(\"%d sentences loaded.\" % len(self.sentences))\n",
        "\n",
        "        if words_vocab is None:\n",
        "            words_vocab = vocab(word_cnt, specials=[PAD, UNKNOWN])\n",
        "            words_vocab.set_default_index(words_vocab[UNKNOWN])\n",
        "\n",
        "        self.words_vocab = words_vocab\n",
        "\n",
        "        self.unknown_idx = self.words_vocab[UNKNOWN]\n",
        "        self.pad_idx = self.words_vocab[PAD]\n",
        "\n",
        "        if tags_vocab is None:\n",
        "            tags_vocab = vocab(tag_cnt, specials=[])\n",
        "        self.tags_vocab = tags_vocab\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Sentence:\n",
        "        \"\"\"\n",
        "        Get the idx'th sentence in the dataset.\n",
        "        \"\"\"\n",
        "        return self.sentences[idx]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"\n",
        "        Return the number of sentences in the dataset.\n",
        "        \"\"\"\n",
        "        # TODO: Implement this method\n",
        "        # START HERE\n",
        "        return len(self.sentences)\n",
        "        #         raise NotImplementedError\n",
        "        # END\n",
        "\n",
        "    def form_batch(self, sentences: List[Sentence]) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        A customized function for batching a number of sentences together.\n",
        "        Different sentences have different lengths. Let max_len be the longest length.\n",
        "        When packing them into one tensor, we need to pad all sentences to max_len.\n",
        "        Return values:\n",
        "            `words`: a list in which each element itself is a list of words in a sentence\n",
        "            `word_idxs`: a batch_size x max_len tensor.\n",
        "                       word_idxs[i][j] is the index of the j'th word in the i'th sentence .\n",
        "            `tags`: a list in which each element itself is a list of tags in a sentence\n",
        "            `tag_idxs`: a batch_size x max_len tensor\n",
        "                      tag_idxs[i][j] is the index of the j'th tag in the i'th sentence.\n",
        "            `valid_mask`: a batch_size x max_len tensor\n",
        "                        valid_mask[i][j] is True if the i'th sentence has the j'th word.\n",
        "                        Otherwise, valid[i][j] is False.\n",
        "        \"\"\"\n",
        "        words: List[List[str]] = []\n",
        "        tags: List[List[str]] = []\n",
        "        max_len = -1  # length of the longest sentence\n",
        "        for sent in sentences:\n",
        "            words.append([])\n",
        "            tags.append([])\n",
        "            for w, t in sent:\n",
        "                words[-1].append(w)\n",
        "                tags[-1].append(t)\n",
        "            max_len = max(max_len, len(words[-1]))\n",
        "\n",
        "        batch_size = len(sentences)\n",
        "        word_idxs = torch.full(\n",
        "            (batch_size, max_len), fill_value=self.pad_idx, dtype=torch.int64\n",
        "        )\n",
        "        tag_idxs = torch.full_like(word_idxs, fill_value=self.tags_vocab[\"O\"])\n",
        "        valid_mask = torch.zeros_like(word_idxs, dtype=torch.bool)\n",
        "\n",
        "        ## TODO: Fill in the values in word_idxs, tag_idxs, and valid_mask\n",
        "        ## Caveat: There may be out-of-vocabulary words in validation data\n",
        "        ## See torchtext.vocab.Vocab: https://pytorch.org/text/stable/vocab.html#torchtext.vocab.Vocab\n",
        "        ## START HERE\n",
        "        for i, sent in enumerate(sentences):\n",
        "          for j, (w, t) in enumerate(sent):\n",
        "              word_idx = self.words_vocab[w] if w in self.words_vocab else self.unknown_idx\n",
        "              tag_idxs[i][j] = self.tags_vocab[t]\n",
        "              valid_mask[i][j] = word_idx != self.unknown_idx\n",
        "              word_idxs[i][j] = word_idx\n",
        "\n",
        "\n",
        "        # raise NotImplementedError\n",
        "\n",
        "        # END\n",
        "\n",
        "        return {\n",
        "            \"words\": words,\n",
        "            \"word_idxs\": word_idxs,\n",
        "            \"tags\": tags,\n",
        "            \"tag_idxs\": tag_idxs,\n",
        "            \"valid_mask\": valid_mask,\n",
        "        }\n",
        "\n",
        "\n",
        "def create_sequence_dataloaders(\n",
        "    batch_size: int, shuffle: bool = True\n",
        ") -> Tuple[DataLoader, DataLoader, Vocab]:\n",
        "    \"\"\"\n",
        "    Create the dataloaders for training and validaiton.\n",
        "    \"\"\"\n",
        "    ds_train = SequenceDataset(\"eng.train\")\n",
        "    ds_val = SequenceDataset(\"eng.val\", words_vocab=ds_train.words_vocab, tags_vocab=ds_train.tags_vocab)\n",
        "    loader_train = DataLoader(\n",
        "        ds_train,\n",
        "        batch_size,\n",
        "        shuffle,\n",
        "        collate_fn=ds_train.form_batch,  # customized function for batching\n",
        "        drop_last=True,\n",
        "        pin_memory=True,\n",
        "    )\n",
        "    loader_val = DataLoader(\n",
        "        ds_val, batch_size, collate_fn=ds_val.form_batch, pin_memory=True\n",
        "    )\n",
        "    return loader_train, loader_val, ds_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2EcVxYuYvGv"
      },
      "source": [
        "Here is a simple sanity-check. Try to understand its output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TazmodGWYx2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "077cc9ce-6db7-44f4-d6e8-d8e89030086e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "Iterating on the training data..\n",
            "{'words': [['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb', '.'], ['Peter', 'Blackburn'], ['BRUSSELS', '0000-00-00']], 'word_idxs': tensor([[ 2,  3,  4,  5,  6,  7,  8,  9, 10],\n",
            "        [11, 12,  0,  0,  0,  0,  0,  0,  0],\n",
            "        [13, 14,  0,  0,  0,  0,  0,  0,  0]]), 'tags': [['ORG', 'O', 'MISC', 'O', 'O', 'O', 'MISC', 'O', 'O'], ['PER', 'PER'], ['LOC', 'O']], 'tag_idxs': tensor([[0, 1, 2, 1, 1, 1, 2, 1, 1],\n",
            "        [3, 3, 1, 1, 1, 1, 1, 1, 1],\n",
            "        [4, 1, 1, 1, 1, 1, 1, 1, 1]]), 'valid_mask': tensor([[ True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
            "        [ True,  True, False, False, False, False, False, False, False],\n",
            "        [ True,  True, False, False, False, False, False, False, False]])}\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "def check_sequence_dataloader() -> None:\n",
        "    loader_train, _, _ = create_sequence_dataloaders(batch_size=3, shuffle=False)\n",
        "    print(\"Iterating on the training data..\")\n",
        "    for i, data_batch in enumerate(loader_train):\n",
        "        if i == 0:\n",
        "            print(data_batch)\n",
        "    print(\"Done!\")\n",
        "\n",
        "\n",
        "check_sequence_dataloader()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifk3i-obY8YB"
      },
      "source": [
        "### Implement the Model **(8 points)**\n",
        "\n",
        "Next, implement LSTM for predicting NER tags from input words. [nn.LSTM](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html#torch.nn.LSTM) is definitely useful. Further, it is tricky to handle sentences in the same batch with different lengths. Please read the PyTorch documentation in detail!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3V0NvQynZF8e"
      },
      "outputs": [],
      "source": [
        "class LSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Long short-term memory for NER\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, words_vocab: Vocab, tags_vocab:Vocab, d_emb: int, d_hidden: int, bidirectional: bool) -> None:\n",
        "        \"\"\"\n",
        "        Initialize an LSTM\n",
        "        Parameters:\n",
        "            `words_vocab`: vocabulary of words\n",
        "            `tags_vocab`: vocabulary of tags\n",
        "            `d_emb`: dimension of word embeddings (D)\n",
        "            `d_hidden`: dimension of the hidden layer (H)\n",
        "            `bidirectional`: true if LSTM should be bidirectional\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # TODO: Create the word embeddings (nn.Embedding),\n",
        "        #       the LSTM (nn.LSTM) and the output layer (nn.Linear).\n",
        "        #       Read the torch docs for additional guidance : https://pytorch.org/docs/stable\n",
        "        #       Note: Pay attention to the LSTM output shapes!\n",
        "        # START HERE\n",
        "        self.embedding = nn.Embedding(len(words_vocab), d_emb)\n",
        "        self.lstm = nn.LSTM(d_emb, d_hidden, bidirectional=bidirectional, batch_first=True)\n",
        "        self.linear = nn.Linear(d_hidden * 2 if bidirectional else d_hidden, len(tags_vocab))\n",
        "        # raise NotImplementedError\n",
        "\n",
        "        # END\n",
        "\n",
        "    def forward(\n",
        "        self, word_idxs: torch.Tensor, valid_mask: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Given words in sentences, predict the logits of the NER tag.\n",
        "        Parameters:\n",
        "            `word_idxs`: a batch_size x max_len tensor\n",
        "            `valid_mask`: a batch_size x max_len tensor\n",
        "        Return values:\n",
        "            `logits`: a batch_size x max_len x 5 tensor\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass\n",
        "        # START HERE\n",
        "        embedded = self.embedding(word_idxs)\n",
        "        lstm_output, _ = self.lstm(embedded)\n",
        "        logits = self.linear(lstm_output)\n",
        "\n",
        "        # Apply mask to ignore padded positions\n",
        "        logits[~valid_mask] = float('-inf')\n",
        "        # raise NotImplementedError\n",
        "\n",
        "        # END\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BFTKaB4Zydx"
      },
      "source": [
        "We do a sanity-check by loading a batch of data examples and pass it through the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PKg1ni4QZ6D1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "16d9a81a-89dd-44d7-e18c-26e6a98cd1de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (embedding): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n",
            "Input word_idxs shape: torch.Size([4, 24])\n",
            "Input valid_mask shape: torch.Size([4, 24])\n",
            "Output logits shape: torch.Size([4, 24, 5])\n"
          ]
        }
      ],
      "source": [
        "def check_lstm() -> None:\n",
        "    # Hyperparameters\n",
        "    batch_size = 4\n",
        "    d_emb = 64\n",
        "    d_hidden = 128\n",
        "    bidirectional = True\n",
        "    # Create the dataloaders and the model\n",
        "    loader_train, _, ds_train = create_sequence_dataloaders(batch_size)\n",
        "    model = LSTM(ds_train.words_vocab, ds_train.tags_vocab, d_emb, d_hidden, bidirectional)\n",
        "    device = get_device()\n",
        "    model.to(device)\n",
        "    print(model)\n",
        "    # Get the first batch\n",
        "    data_batch = next(iter(loader_train))\n",
        "    # Move data to GPU\n",
        "    word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "    tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "    valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "    # Calculate the model\n",
        "    print(\"Input word_idxs shape:\", word_idxs.size())\n",
        "    print(\"Input valid_mask shape:\", valid_mask.size())\n",
        "    logits = model(word_idxs, valid_mask)\n",
        "    print(\"Output logits shape:\", logits.size())\n",
        "\n",
        "\n",
        "check_lstm()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jddDYUiLY-hc"
      },
      "source": [
        "### Training and Validation **(6 points)**\n",
        "\n",
        "Complete the functions for training and validating the LSTM model. When calculating the loss function, you only want to include values from valid positions (where `valid_mask` is `True`). The `reduction` parameter in [F.cross_entropy](https://pytorch.org/docs/stable/nn.functional.html#torch.nn.functional.cross_entropy) may be useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hv_15mnXZ_dy"
      },
      "outputs": [],
      "source": [
        "def train_lstm(\n",
        "    model: nn.Module,\n",
        "    loader: DataLoader,\n",
        "    optimizer: optim.Optimizer,\n",
        "    device: torch.device,\n",
        "    silent: bool = False,  # whether to print the training loss\n",
        ") -> Tuple[float, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Train the LSTM model.\n",
        "    Return values:\n",
        "        1. the average training loss\n",
        "        2. training metrics such as accuracy and F1 score\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    ground_truth = []\n",
        "    predictions = []\n",
        "    losses = []\n",
        "    report_interval = 100\n",
        "\n",
        "    for i, data_batch in enumerate(loader):\n",
        "        word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "        tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "        valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "\n",
        "        # TODO: Do the same tasks as train_ffnn\n",
        "        # 1. Perform the forward pass to calculate the model's output. Save it to the variable \"logits\".\n",
        "        # 2. Calculate the loss using the output and the ground truth tags. Save it to the variable \"loss\".\n",
        "        # 3. Perform the backward pass to calculate the gradient.\n",
        "        # 4. Use the optimizer to update model parameters.\n",
        "        # Caveat: You may need to call optimizer.zero_grad(). Figure out what it does!\n",
        "        # Caveat: When calculating the loss, you should only consider positions where valid_mask == True\n",
        "        # START HERE\n",
        "        logits = model(word_idxs, valid_mask)  # Forward pass\n",
        "        loss = F.cross_entropy(logits[valid_mask], tag_idxs[valid_mask], reduction='mean')\n",
        "        # raise NotImplementedError\n",
        "        # END\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        # we get (unmasked) predictions by getting argmax of logits along last dimension (You will need to define logits!)\n",
        "        net_predictions = torch.argmax(logits, -1)\n",
        "\n",
        "        # flattening a tensor simply converts it from a multi-dimensional to a single-dimensional tensor; we flatten here to make it easier to extract ground truths and predictions\n",
        "        tag_idxs_flat = tag_idxs.flatten()\n",
        "        valid_mask_flat = valid_mask.flatten()\n",
        "        net_predictions_flat = net_predictions.flatten()\n",
        "\n",
        "        ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n",
        "        predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n",
        "\n",
        "        if not silent and i > 0 and i % report_interval == 0:\n",
        "            print(\n",
        "                \"\\t[%06d/%06d] Loss: %f\"\n",
        "                % (i, len(loader), np.mean(losses[-report_interval:]))\n",
        "            )\n",
        "\n",
        "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
        "\n",
        "\n",
        "def validate_lstm(\n",
        "    model: nn.Module, loader: DataLoader, device: torch.device\n",
        ") -> Tuple[float, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Validate the model.\n",
        "    Return the validation loss and metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    ground_truth = []\n",
        "    predictions = []\n",
        "    losses = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for data_batch in loader:\n",
        "            word_idxs = data_batch[\"word_idxs\"].to(device, non_blocking=True)\n",
        "            tag_idxs = data_batch[\"tag_idxs\"].to(device, non_blocking=True)\n",
        "            valid_mask = data_batch[\"valid_mask\"].to(device, non_blocking=True)\n",
        "\n",
        "            # TODO: Do the same tasks as validate_ffnn\n",
        "            # 1. Perform the forward pass to calculate the model's output. Save it to the variable \"logits\".\n",
        "            # 2. Calculate the loss using the output and the ground truth tags. Save it to the variable \"loss\".\n",
        "            # Caveat: When calculating the loss, you should only consider positions where valid_mask == True\n",
        "\n",
        "            # START HERE\n",
        "            logits = model(word_idxs, valid_mask)  # Forward pass\n",
        "            loss = F.cross_entropy(logits[valid_mask], tag_idxs[valid_mask], reduction='mean')\n",
        "            # raise NotImplementedError\n",
        "            # END\n",
        "\n",
        "            losses.append(loss.item())\n",
        "\n",
        "            # we get (unmasked) predictions by getting argmax of logits (You will need to define logits!)\n",
        "            net_predictions = torch.argmax(logits, -1)\n",
        "\n",
        "            # flattening a tensor simply converts it from a multi-dimensional to a single-dimensional tensor; we flatten here to make it easier to extract ground truths and predictions\n",
        "            tag_idxs_flat = tag_idxs.flatten()\n",
        "            valid_mask_flat = valid_mask.flatten()\n",
        "            net_predictions_flat = net_predictions.flatten()\n",
        "\n",
        "            ground_truth.extend(tag_idxs_flat[valid_mask_flat].tolist())\n",
        "            predictions.extend(net_predictions_flat[valid_mask_flat].tolist())\n",
        "\n",
        "    return np.mean(losses), eval_metrics(ground_truth, predictions)\n",
        "\n",
        "\n",
        "def train_val_loop_lstm(hyperparams: Dict[str, Any]) -> None:\n",
        "    \"\"\"\n",
        "    Train and validate the LSTM model for a number of epochs.\n",
        "    \"\"\"\n",
        "    print(\"Hyperparameters:\", hyperparams)\n",
        "    # Create the dataloaders\n",
        "    loader_train, loader_val, ds_train = create_sequence_dataloaders(\n",
        "        hyperparams[\"batch_size\"]\n",
        "    )\n",
        "    # Create the model\n",
        "    model = LSTM(\n",
        "        ds_train.words_vocab,\n",
        "        ds_train.tags_vocab,\n",
        "        hyperparams[\"d_emb\"],\n",
        "        hyperparams[\"d_hidden\"],\n",
        "        hyperparams[\"bidirectional\"],\n",
        "    )\n",
        "    device = get_device()\n",
        "    model.to(device)\n",
        "    print(model)\n",
        "    # Create the optimizer\n",
        "    optimizer = optim.RMSprop(\n",
        "        model.parameters(), hyperparams[\"learning_rate\"], weight_decay=hyperparams[\"l2\"]\n",
        "    )\n",
        "\n",
        "    # Train and validate\n",
        "    for i in range(hyperparams[\"num_epochs\"]):\n",
        "        print(\"Epoch #%d\" % i)\n",
        "\n",
        "        print(\"Training..\")\n",
        "        loss_train, metrics_train = train_lstm(model, loader_train, optimizer, device)\n",
        "        print(\"Training loss: \", loss_train)\n",
        "        print(\"Training metrics:\")\n",
        "        for k, v in metrics_train.items():\n",
        "            print(\"\\t\", k, \": \", v)\n",
        "\n",
        "        print(\"Validating..\")\n",
        "        loss_val, metrics_val = validate_lstm(model, loader_val, device)\n",
        "        print(\"Validation loss: \", loss_val)\n",
        "        print(\"Validation metrics:\")\n",
        "        for k, v in metrics_val.items():\n",
        "            print(\"\\t\", k, \": \", v)\n",
        "\n",
        "    print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU9Nef7yal_M"
      },
      "source": [
        "Run the experiment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFxQxlokai6Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92920922-eb39-4fb0-c0c6-9bca20f9b9d2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: {'bidirectional': True, 'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'num_epochs': 15, 'learning_rate': 0.005, 'l2': 1e-06}\n",
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (embedding): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128, batch_first=True, bidirectional=True)\n",
            "  (linear): Linear(in_features=256, out_features=5, bias=True)\n",
            ")\n",
            "Epoch #0\n",
            "Training..\n",
            "Training loss:  0.7012706156130191\n",
            "Training metrics:\n",
            "\t accuracy :  0.8181550779477822\n",
            "\t f1 :  [0.1778207  0.90305613 0.05534455 0.30316102 0.29261724]\n",
            "\t average f1 :  0.34639992666335023\n",
            "\t confusion matrix :  [[  1565   7584     64    305    340]\n",
            " [  5084 158294    695   1117   1953]\n",
            " [   303   3773    153    101    176]\n",
            " [   443   8009     67   2249    173]\n",
            " [   349   5771     44    124   1847]]\n",
            "Validating..\n",
            "Validation loss:  0.24463607370853424\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9332133173311997\n",
            "\t f1 :  [0.563125   0.9751931  0.37364621 0.64573991 0.72041375]\n",
            "\t average f1 :  0.6556235934350712\n",
            "\t confusion matrix :  [[  901   532    37   169   157]\n",
            " [  120 38761    23   162    74]\n",
            " [  181   302   207    56    79]\n",
            " [   71   429     4   936    47]\n",
            " [  131   330    12    89  1184]]\n",
            "Epoch #1\n",
            "Training..\n",
            "Training loss:  0.20811987971818005\n",
            "Training metrics:\n",
            "\t accuracy :  0.9368063273412309\n",
            "\t f1 :  [0.66480009 0.97630722 0.58519421 0.77652523 0.76702509]\n",
            "\t average f1 :  0.7539703682072396\n",
            "\t confusion matrix :  [[  5936   2235    361    697    673]\n",
            " [   414 165858    143    478    241]\n",
            " [   598   1209   2162    219    323]\n",
            " [   451   2174     61   8025    238]\n",
            " [   557   1156    151    301   5992]]\n",
            "Validating..\n",
            "Validation loss:  0.14019991989646638\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9597501889140775\n",
            "\t f1 :  [0.70847794 0.98896185 0.6637037  0.81385562 0.82937365]\n",
            "\t average f1 :  0.8008745520006133\n",
            "\t confusion matrix :  [[ 1429   171    21    99    76]\n",
            " [  229 38705    34   141    31]\n",
            " [  224    87   448    36    30]\n",
            " [  109   102     5  1257    14]\n",
            " [  247    69    17    69  1344]]\n",
            "Epoch #2\n",
            "Training..\n",
            "Training loss:  0.10687976164950265\n",
            "Training metrics:\n",
            "\t accuracy :  0.9680427028510139\n",
            "\t f1 :  [0.81979709 0.99032198 0.77689531 0.90422131 0.86596649]\n",
            "\t average f1 :  0.8714404347994369\n",
            "\t confusion matrix :  [[  7959    850    280    358    452]\n",
            " [   318 166179    120    226    107]\n",
            " [   415    537   3228    133    200]\n",
            " [   304    681     40   9757    132]\n",
            " [   522    409    129    193   6926]]\n",
            "Validating..\n",
            "Validation loss:  0.09506541384117943\n",
            "Validation metrics:\n",
            "\t accuracy :  0.974107658798951\n",
            "\t f1 :  [0.81041725 0.99225063 0.80943739 0.8812343  0.88661552]\n",
            "\t average f1 :  0.8759910170878369\n",
            "\t confusion matrix :  [[ 1447   158    50    32   109]\n",
            " [  101 38925    57    27    30]\n",
            " [   48    69   669     5    34]\n",
            " [   76   121    22  1228    40]\n",
            " [  103    45    30     8  1560]]\n",
            "Epoch #3\n",
            "Training..\n",
            "Training loss:  0.06119682819203094\n",
            "Training metrics:\n",
            "\t accuracy :  0.9825154766715055\n",
            "\t f1 :  [0.89821337 0.99516181 0.85816918 0.95862479 0.92174772]\n",
            "\t average f1 :  0.9263833725368296\n",
            "\t confusion matrix :  [[  8798    435    186    161    310]\n",
            " [   192 166505     81     86     62]\n",
            " [   258    335   3703     75    139]\n",
            " [   149    241     38  10484     61]\n",
            " [   303    187    112     94   7468]]\n",
            "Validating..\n",
            "Validation loss:  0.08188069292477199\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9787971729563942\n",
            "\t f1 :  [0.84178611 0.99351611 0.84216941 0.9137402  0.90866783]\n",
            "\t average f1 :  0.8999759304902943\n",
            "\t confusion matrix :  [[ 1527   119    43    45    62]\n",
            " [  105 38920    53    41    21]\n",
            " [   45    61   691     4    24]\n",
            " [   45    71     8  1340    23]\n",
            " [  110    37    21    16  1562]]\n",
            "Epoch #4\n",
            "Training..\n",
            "Training loss:  0.037718495423043216\n",
            "Training metrics:\n",
            "\t accuracy :  0.989771707706111\n",
            "\t f1 :  [0.93637663 0.99721735 0.9127182  0.98121617 0.95428291]\n",
            "\t average f1 :  0.9563622531193697\n",
            "\t confusion matrix :  [[  9169    276    134     99    174]\n",
            " [   136 166821     57     33     30]\n",
            " [   144    216   4026     34     98]\n",
            " [    85     80     14  10787     28]\n",
            " [   198    103     73     40   7765]]\n",
            "Validating..\n",
            "Validation loss:  0.07929027027317456\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9802195848335333\n",
            "\t f1 :  [0.85108726 0.99434596 0.84528749 0.91970803 0.91162791]\n",
            "\t average f1 :  0.904411328098837\n",
            "\t confusion matrix :  [[ 1546   112    44    29    65]\n",
            " [   87 38954    61    23    15]\n",
            " [   40    48   713     2    22]\n",
            " [   57    68    15  1323    24]\n",
            " [  107    29    29    13  1568]]\n",
            "Epoch #5\n",
            "Training..\n",
            "Training loss:  0.024558672129555984\n",
            "Training metrics:\n",
            "\t accuracy :  0.993680301109192\n",
            "\t f1 :  [0.96027196 0.99819656 0.94836528 0.99107224 0.9697787 ]\n",
            "\t average f1 :  0.9735369473058366\n",
            "\t confusion matrix :  [[  9463    182     76     47    125]\n",
            " [   105 166602     36     16     25]\n",
            " [    77    140   4206     18     60]\n",
            " [    35     35      6  10879     21]\n",
            " [   136     63     45     18   7910]]\n",
            "Validating..\n",
            "Validation loss:  0.07807193909372602\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9814197448548695\n",
            "\t f1 :  [0.86152987 0.99443721 0.86470235 0.91927627 0.9189344 ]\n",
            "\t average f1 :  0.911776020804169\n",
            "\t confusion matrix :  [[ 1543   117    38    26    72]\n",
            " [   73 38971    51    24    21]\n",
            " [   30    49   719     2    25]\n",
            " [   58    74    11  1321    23]\n",
            " [   82    27    19    14  1604]]\n",
            "Epoch #6\n",
            "Training..\n",
            "Training loss:  0.014963305520790594\n",
            "Training metrics:\n",
            "\t accuracy :  0.9965481980296795\n",
            "\t f1 :  [0.98       0.99887449 0.96858289 0.99561363 0.98553744]\n",
            "\t average f1 :  0.9857216905368619\n",
            "\t confusion matrix :  [[  9653    107     44     19     43]\n",
            " [    56 166847     34     10      9]\n",
            " [    50     92   4347      3     35]\n",
            " [    22     17      2  10895      9]\n",
            " [    53     51     22     14   8041]]\n",
            "Validating..\n",
            "Validation loss:  0.08150781424982208\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9816642218962528\n",
            "\t f1 :  [0.86149506 0.9943755  0.87112172 0.92163009 0.92060762]\n",
            "\t average f1 :  0.9138460004077953\n",
            "\t confusion matrix :  [[ 1527   129    39    28    73]\n",
            " [   70 38983    48    19    20]\n",
            " [   23    49   730     3    20]\n",
            " [   49    77    14  1323    24]\n",
            " [   80    29    20    11  1606]]\n",
            "Epoch #7\n",
            "Training..\n",
            "Training loss:  0.009900777928393197\n",
            "Training metrics:\n",
            "\t accuracy :  0.9979783761118931\n",
            "\t f1 :  [0.98831386 0.99928395 0.98260482 0.99789551 0.9915327 ]\n",
            "\t average f1 :  0.9919261674756674\n",
            "\t confusion matrix :  [[  9768     68     23      9     22]\n",
            " [    41 166769     18      2     10]\n",
            " [    24     58   4406      3     20]\n",
            " [    12      9      0  10906      5]\n",
            " [    32     33     10      6   8080]]\n",
            "Validating..\n",
            "Validation loss:  0.08726235372679574\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9819975996799574\n",
            "\t f1 :  [0.86352077 0.99410274 0.88235294 0.92243187 0.92450639]\n",
            "\t average f1 :  0.9173829401400682\n",
            "\t confusion matrix :  [[ 1528   152    30    25    61]\n",
            " [   51 39024    39    14    12]\n",
            " [   27    58   720     3    17]\n",
            " [   44   102     5  1320    16]\n",
            " [   93    35    13    13  1592]]\n",
            "Epoch #8\n",
            "Training..\n",
            "Training loss:  0.006808973296924874\n",
            "Training metrics:\n",
            "\t accuracy :  0.9987696200766123\n",
            "\t f1 :  [0.99360341 0.9995427  0.98880886 0.99872263 0.99472004]\n",
            "\t average f1 :  0.9950795281544419\n",
            "\t confusion matrix :  [[  9786     41     13      6     10]\n",
            " [    17 167209     14      3      6]\n",
            " [    16     38   4462      0      9]\n",
            " [     5      7      1  10946      2]\n",
            " [    18     27     10      4   8101]]\n",
            "Validating..\n",
            "Validation loss:  0.08650173619389534\n",
            "Validation metrics:\n",
            "\t accuracy :  0.983308885629195\n",
            "\t f1 :  [0.8718687  0.99471075 0.89051095 0.92787795 0.9279637 ]\n",
            "\t average f1 :  0.9225864087735974\n",
            "\t confusion matrix :  [[ 1514   137    32    31    82]\n",
            " [   47 39023    39    14    17]\n",
            " [   20    50   732     3    20]\n",
            " [   33    86     5  1338    25]\n",
            " [   63    25    11    11  1636]]\n",
            "Epoch #9\n",
            "Training..\n",
            "Training loss:  0.012698288498170398\n",
            "Training metrics:\n",
            "\t accuracy :  0.9967751743250046\n",
            "\t f1 :  [0.97452845 0.99911139 0.98403194 0.99595436 0.98390945]\n",
            "\t average f1 :  0.9875071177835582\n",
            "\t confusion matrix :  [[  9584    100     30     42    108]\n",
            " [    78 166967     22      7     12]\n",
            " [    30     40   4437      2     13]\n",
            " [    19     13      1  10955      2]\n",
            " [    94     25      6      3   8041]]\n",
            "Validating..\n",
            "Validation loss:  0.07977567187377385\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9838867404542828\n",
            "\t f1 :  [0.87591653 0.9950363  0.89143546 0.92999314 0.93230506]\n",
            "\t average f1 :  0.9249372969955448\n",
            "\t confusion matrix :  [[ 1553   108    34    29    72]\n",
            " [   60 38990    46    27    17]\n",
            " [   22    42   739     3    19]\n",
            " [   47    64     6  1355    15]\n",
            " [   68    25     8    13  1632]]\n",
            "Epoch #10\n",
            "Training..\n",
            "Training loss:  0.004827444683070536\n",
            "Training metrics:\n",
            "\t accuracy :  0.999162337079548\n",
            "\t f1 :  [0.99472349 0.99966178 0.99469965 0.9991327  0.99681685]\n",
            "\t average f1 :  0.9970068931589164\n",
            "\t confusion matrix :  [[  9803     34      3      7     10]\n",
            " [    25 166997      9      2      3]\n",
            " [     7     22   4504      0      5]\n",
            " [     5      2      0  10944      3]\n",
            " [    13     16      2      0   8142]]\n",
            "Validating..\n",
            "Validation loss:  0.08656044091497149\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9836644885984798\n",
            "\t f1 :  [0.87676653 0.99482077 0.88729875 0.93056995 0.93176606]\n",
            "\t average f1 :  0.924244410743353\n",
            "\t confusion matrix :  [[ 1551   119    37    24    65]\n",
            " [   58 38992    49    25    16]\n",
            " [   19    42   744     2    18]\n",
            " [   44    71     7  1347    18]\n",
            " [   70    26    15    10  1625]]\n",
            "Epoch #11\n",
            "Training..\n",
            "Training loss:  0.002562287461909431\n",
            "Training metrics:\n",
            "\t accuracy :  0.9996853461192688\n",
            "\t f1 :  [0.99853187 0.99984705 0.99756637 0.9996802  0.9989584 ]\n",
            "\t average f1 :  0.9989167779305193\n",
            "\t confusion matrix :  [[  9862     15      0      3      1]\n",
            " [     6 166693      4      0      4]\n",
            " [     2     13   4509      1      2]\n",
            " [     1      1      0  10941      1]\n",
            " [     1      8      0      0   8152]]\n",
            "Validating..\n",
            "Validation loss:  0.09234096322740827\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9835978130417389\n",
            "\t f1 :  [0.87911464 0.9947314  0.88454707 0.92941176 0.9319222 ]\n",
            "\t average f1 :  0.9239454153733112\n",
            "\t confusion matrix :  [[ 1549   119    38    25    65]\n",
            " [   57 38988    56    23    16]\n",
            " [   18    41   747     2    17]\n",
            " [   42    72     7  1343    23]\n",
            " [   62    29    16    10  1629]]\n",
            "Epoch #12\n",
            "Training..\n",
            "Training loss:  0.0018582883791101199\n",
            "Training metrics:\n",
            "\t accuracy :  0.9998005057180048\n",
            "\t f1 :  [0.99899071 0.99991615 0.99911524 0.99958966 0.99908049]\n",
            "\t average f1 :  0.9993384494011945\n",
            "\t confusion matrix :  [[  9898     10      0      4      0]\n",
            " [     2 166941      2      0      3]\n",
            " [     0      4   4517      0      1]\n",
            " [     2      0      1  10962      1]\n",
            " [     2      7      0      1   8149]]\n",
            "Validating..\n",
            "Validation loss:  0.09693628283483642\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9831755345157132\n",
            "\t f1 :  [0.87693188 0.9944426  0.88363851 0.92770668 0.92997685]\n",
            "\t average f1 :  0.9225393051595798\n",
            "\t confusion matrix :  [[ 1532   136    43    26    59]\n",
            " [   48 39009    51    20    12]\n",
            " [   16    43   748     3    15]\n",
            " [   34    88     7  1341    17]\n",
            " [   68    38    19    14  1607]]\n",
            "Epoch #13\n",
            "Training..\n",
            "Training loss:  0.0014308356179821272\n",
            "Training metrics:\n",
            "\t accuracy :  0.9998651974856835\n",
            "\t f1 :  [0.99939148 0.99994005 0.99944904 0.99972545 0.99932602]\n",
            "\t average f1 :  0.9995664079576073\n",
            "\t confusion matrix :  [[  9854      6      0      1      0]\n",
            " [     3 166798      1      0      2]\n",
            " [     0      2   4535      1      1]\n",
            " [     2      0      0  10924      1]\n",
            " [     0      6      0      1   8155]]\n",
            "Validating..\n",
            "Validation loss:  0.09851103914635521\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9833533360003556\n",
            "\t f1 :  [0.87717298 0.99458053 0.88544153 0.92778542 0.93091537]\n",
            "\t average f1 :  0.9231791670680172\n",
            "\t confusion matrix :  [[ 1539   128    38    28    63]\n",
            " [   50 38998    52    24    16]\n",
            " [   19    44   742     6    14]\n",
            " [   36    78     6  1349    18]\n",
            " [   69    33    13    14  1617]]\n",
            "Epoch #14\n",
            "Training..\n",
            "Training loss:  0.0011494124536747458\n",
            "Training metrics:\n",
            "\t accuracy :  0.9998952216063026\n",
            "\t f1 :  [0.99934201 0.99995807 0.99977807 0.99977139 0.99951016]\n",
            "\t average f1 :  0.9996719395842604\n",
            "\t confusion matrix :  [[  9872      5      0      3      0]\n",
            " [     2 166930      0      0      1]\n",
            " [     0      1   4505      0      0]\n",
            " [     1      0      1  10933      0]\n",
            " [     2      5      0      0   8162]]\n",
            "Validating..\n",
            "Validation loss:  0.10275686106511525\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9833311108147753\n",
            "\t f1 :  [0.88029782 0.99444246 0.88823182 0.92564281 0.93007194]\n",
            "\t average f1 :  0.9237373701348858\n",
            "\t confusion matrix :  [[ 1537   131    42    24    62]\n",
            " [   43 39008    52    20    17]\n",
            " [   16    41   751     3    14]\n",
            " [   35    94     6  1332    20]\n",
            " [   65    38    15    12  1616]]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "train_val_loop_lstm({\n",
        "    \"bidirectional\": True,\n",
        "    \"batch_size\": 512,\n",
        "    \"d_emb\": 64,\n",
        "    \"d_hidden\": 128,\n",
        "    \"num_epochs\": 15,\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"l2\": 1e-6,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6vA-Yjqg7n0V"
      },
      "source": [
        "We were using bidirectional LSTMs. Please re-run the experiment with a regular (unidirectional) LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7wNrdvJ98ARB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74b3c96e-dafb-4938-8106-c4b0065ae536"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hyperparameters: {'bidirectional': False, 'batch_size': 512, 'd_emb': 64, 'd_hidden': 128, 'num_epochs': 15, 'learning_rate': 0.005, 'l2': 1e-06}\n",
            "Loading data from eng.train\n",
            "14041 sentences loaded.\n",
            "Loading data from eng.val\n",
            "3490 sentences loaded.\n",
            "LSTM(\n",
            "  (embedding): Embedding(20102, 64)\n",
            "  (lstm): LSTM(64, 128, batch_first=True)\n",
            "  (linear): Linear(in_features=128, out_features=5, bias=True)\n",
            ")\n",
            "Epoch #0\n",
            "Training..\n",
            "Training loss:  0.5027170368918666\n",
            "Training metrics:\n",
            "\t accuracy :  0.8343376646679725\n",
            "\t f1 :  [0.20176789 0.91789578 0.07832898 0.38162944 0.38703111]\n",
            "\t average f1 :  0.39333064179121013\n",
            "\t confusion matrix :  [[  1769   6636    133    557    784]\n",
            " [  4161 159690   1035    860   1335]\n",
            " [   604   3097    240    170    403]\n",
            " [   513   6920    119   3033    407]\n",
            " [   609   4524     87    283   2662]]\n",
            "Validating..\n",
            "Validation loss:  0.21627393790653773\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9337689469707072\n",
            "\t f1 :  [0.42442293 0.97849395 0.43906376 0.6774083  0.70253694]\n",
            "\t average f1 :  0.6443851755713521\n",
            "\t confusion matrix :  [[  570   617    87   188   334]\n",
            " [   65 38924    23    83    45]\n",
            " [  107   282   272    38   126]\n",
            " [   53   363     7   988    76]\n",
            " [   95   233    25   133  1260]]\n",
            "Epoch #1\n",
            "Training..\n",
            "Training loss:  0.20186013755974946\n",
            "Training metrics:\n",
            "\t accuracy :  0.9344616798345935\n",
            "\t f1 :  [0.61980492 0.9800775  0.54341137 0.78368379 0.72954248]\n",
            "\t average f1 :  0.7313040124166935\n",
            "\t confusion matrix :  [[  5846   1832    440    725    996]\n",
            " [   590 165023    183    618    263]\n",
            " [   941    902   2031    233    420]\n",
            " [   690   1554     82   8367    320]\n",
            " [   958    767    212    397   5844]]\n",
            "Validating..\n",
            "Validation loss:  0.13441905592169082\n",
            "Validation metrics:\n",
            "\t accuracy :  0.960461394852647\n",
            "\t f1 :  [0.66169799 0.98947021 0.67329047 0.82865833 0.82329432]\n",
            "\t average f1 :  0.7952822643869342\n",
            "\t confusion matrix :  [[ 1021   284   161   119   211]\n",
            " [   56 38950    61    55    18]\n",
            " [   62   126   576    17    44]\n",
            " [   41   155    23  1226    42]\n",
            " [  110    74    65    55  1442]]\n",
            "Epoch #2\n",
            "Training..\n",
            "Training loss:  0.11644715788187804\n",
            "Training metrics:\n",
            "\t accuracy :  0.9632182530168625\n",
            "\t f1 :  [0.77056815 0.99046179 0.7258216  0.89907752 0.84082594]\n",
            "\t average f1 :  0.8453509986831605\n",
            "\t confusion matrix :  [[  7446    886    469    398    654]\n",
            " [   373 165731    152    320    119]\n",
            " [   615    475   3092    119    239]\n",
            " [   359    559     72   9795    155]\n",
            " [   680    308    195    217   6780]]\n",
            "Validating..\n",
            "Validation loss:  0.10526092669793538\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9706849802195848\n",
            "\t f1 :  [0.75647964 0.99284813 0.7701005  0.87404453 0.86113537]\n",
            "\t average f1 :  0.8509216340165737\n",
            "\t confusion matrix :  [[ 1328   180    65    90   133]\n",
            " [   96 38940    40    54    10]\n",
            " [   76    78   613    20    38]\n",
            " [   57    73    13  1315    29]\n",
            " [  158    30    36    43  1479]]\n",
            "Epoch #3\n",
            "Training..\n",
            "Training loss:  0.07491212821117153\n",
            "Training metrics:\n",
            "\t accuracy :  0.9769494434438131\n",
            "\t f1 :  [0.85234487 0.99468023 0.81916714 0.94516614 0.88962922]\n",
            "\t average f1 :  0.900197520710668\n",
            "\t confusion matrix :  [[  8324    507    329    218    478]\n",
            " [   243 166317    106    164     70]\n",
            " [   394    298   3590     77    173]\n",
            " [   213    233     49  10368     93]\n",
            " [   502    158    159    156   7210]]\n",
            "Validating..\n",
            "Validation loss:  0.09729789942502975\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9740409832422101\n",
            "\t f1 :  [0.77631193 0.99336314 0.7960199  0.89723455 0.88047471]\n",
            "\t average f1 :  0.8686808471529087\n",
            "\t confusion matrix :  [[ 1324   189    69    58   156]\n",
            " [   63 38990    40    36    11]\n",
            " [   55    78   640    10    42]\n",
            " [   57    79    11  1314    26]\n",
            " [  116    25    23    24  1558]]\n",
            "Epoch #4\n",
            "Training..\n",
            "Training loss:  0.05262292010916604\n",
            "Training metrics:\n",
            "\t accuracy :  0.984190807820287\n",
            "\t f1 :  [0.88972995 0.99683659 0.87695951 0.97061095 0.91511108]\n",
            "\t average f1 :  0.9298496167429133\n",
            "\t confusion matrix :  [[  8698    329    263    126    421]\n",
            " [   191 166538     60     73     32]\n",
            " [   265    182   3888     45    149]\n",
            " [   119    105     24  10684     55]\n",
            " [   442     85    103    100   7476]]\n",
            "Validating..\n",
            "Validation loss:  0.09429372102022171\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9752855936347069\n",
            "\t f1 :  [0.79419157 0.99445104 0.79782082 0.90056913 0.87525926]\n",
            "\t average f1 :  0.8724583634367098\n",
            "\t confusion matrix :  [[ 1422   140    64    70   100]\n",
            " [   70 38979    42    41     8]\n",
            " [   68    58   659    15    25]\n",
            " [   56    56    11  1345    19]\n",
            " [  169    20    51    29  1477]]\n",
            "Epoch #5\n",
            "Training..\n",
            "Training loss:  0.03794775240951114\n",
            "Training metrics:\n",
            "\t accuracy :  0.9886710000548741\n",
            "\t f1 :  [0.92072026 0.99781247 0.90633763 0.98289426 0.93608907]\n",
            "\t average f1 :  0.9487707385384029\n",
            "\t confusion matrix :  [[  9076    240    199     81    320]\n",
            " [   146 166718     46     31     27]\n",
            " [   193    131   4040     27    109]\n",
            " [    61     54     23  10745     32]\n",
            " [   323     56    107     65   7609]]\n",
            "Validating..\n",
            "Validation loss:  0.09321630692907742\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9769747077388096\n",
            "\t f1 :  [0.80270194 0.99475585 0.81218274 0.90945406 0.88742102]\n",
            "\t average f1 :  0.8813031227658643\n",
            "\t confusion matrix :  [[ 1426   136    46    64   124]\n",
            " [   68 38981    34    47    10]\n",
            " [   77    56   640    16    36]\n",
            " [   47    44     9  1366    21]\n",
            " [  139    16    22    24  1545]]\n",
            "Epoch #6\n",
            "Training..\n",
            "Training loss:  0.031034146677012795\n",
            "Training metrics:\n",
            "\t accuracy :  0.9908327348176522\n",
            "\t f1 :  [0.93588829 0.9980545  0.9273743  0.9878686  0.94781382]\n",
            "\t average f1 :  0.9593999019653028\n",
            "\t confusion matrix :  [[  9182    207    137     62    249]\n",
            " [   136 166727     49     23     29]\n",
            " [   148    115   4150     14     93]\n",
            " [    54     33      9  10871     27]\n",
            " [   265     58     85     45   7728]]\n",
            "Validating..\n",
            "Validation loss:  0.09191469528845378\n",
            "Validation metrics:\n",
            "\t accuracy :  0.977819264790861\n",
            "\t f1 :  [0.80345959 0.99454541 0.82991556 0.92061374 0.88926843]\n",
            "\t average f1 :  0.887560546316462\n",
            "\t confusion matrix :  [[ 1347   174    64    67   144]\n",
            " [   39 39019    37    31    14]\n",
            " [   32    64   688    10    31]\n",
            " [   32    52     7  1380    16]\n",
            " [  107    17    37    23  1562]]\n",
            "Epoch #7\n",
            "Training..\n",
            "Training loss:  0.02348840505712562\n",
            "Training metrics:\n",
            "\t accuracy :  0.9928460962833624\n",
            "\t f1 :  [0.94829169 0.99857226 0.94206437 0.99236781 0.95780177]\n",
            "\t average f1 :  0.9678195794676976\n",
            "\t confusion matrix :  [[  9298    155    126     36    224]\n",
            " [   101 166809     36     16     26]\n",
            " [   120     81   4244      8     71]\n",
            " [    33     28      6  10857     15]\n",
            " [   219     34     74     25   7808]]\n",
            "Validating..\n",
            "Validation loss:  0.09723681317908424\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9771747344090324\n",
            "\t f1 :  [0.80756396 0.99459943 0.83547401 0.91281374 0.88072855]\n",
            "\t average f1 :  0.8862359369250644\n",
            "\t confusion matrix :  [[ 1452   128    42    61   113]\n",
            " [   86 38951    36    55    12]\n",
            " [   52    50   683    17    23]\n",
            " [   44    41     9  1382    11]\n",
            " [  166    15    40    26  1499]]\n",
            "Epoch #8\n",
            "Training..\n",
            "Training loss:  0.0175317771801794\n",
            "Training metrics:\n",
            "\t accuracy :  0.9946924618006758\n",
            "\t f1 :  [0.96157555 0.99900686 0.95750083 0.99516379 0.96634851]\n",
            "\t average f1 :  0.975919108816725\n",
            "\t confusion matrix :  [[  9472    109     96     22    172]\n",
            " [    78 166981     25      8     22]\n",
            " [    81     50   4337      6     65]\n",
            " [    18     17      5  10906     13]\n",
            " [   181     23     57     17   7897]]\n",
            "Validating..\n",
            "Validation loss:  0.09843780526093074\n",
            "Validation metrics:\n",
            "\t accuracy :  0.977974841089923\n",
            "\t f1 :  [0.81587394 0.99489496 0.83353151 0.91214215 0.88410786]\n",
            "\t average f1 :  0.8881100856495141\n",
            "\t confusion matrix :  [[ 1398   138    50    70   140]\n",
            " [   55 38977    37    54    17]\n",
            " [   35    46   701    16    27]\n",
            " [   36    40    10  1386    15]\n",
            " [  107    13    59    26  1541]]\n",
            "Epoch #9\n",
            "Training..\n",
            "Training loss:  0.014910219754609797\n",
            "Training metrics:\n",
            "\t accuracy :  0.9953730508914489\n",
            "\t f1 :  [0.96513985 0.99918172 0.96571492 0.99612421 0.96957081]\n",
            "\t average f1 :  0.9791462985881072\n",
            "\t confusion matrix :  [[  9524     97     89     19    175]\n",
            " [    63 166676     25      3     20]\n",
            " [    60     35   4380      7     45]\n",
            " [    18     11      3  10923     13]\n",
            " [   167     19     47     11   7918]]\n",
            "Validating..\n",
            "Validation loss:  0.10259997099637985\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9774414366359959\n",
            "\t f1 :  [0.80769231 0.99479379 0.82808717 0.91927952 0.87885847]\n",
            "\t average f1 :  0.8857422523820139\n",
            "\t confusion matrix :  [[ 1428   136    45    58   129]\n",
            " [   71 38980    35    37    17]\n",
            " [   53    50   684    17    21]\n",
            " [   40    47    10  1378    12]\n",
            " [  148    15    53    21  1509]]\n",
            "Epoch #10\n",
            "Training..\n",
            "Training loss:  0.013832581781402783\n",
            "Training metrics:\n",
            "\t accuracy :  0.9956771903232883\n",
            "\t f1 :  [0.96721893 0.99921874 0.96306976 0.99653474 0.97451879]\n",
            "\t average f1 :  0.9801121925758624\n",
            "\t confusion matrix :  [[  9545     86     88     19    143]\n",
            " [    66 166908     28      4     14]\n",
            " [    83     41   4342      6     46]\n",
            " [    16      9      4  10928     10]\n",
            " [   146     13     37      8   7974]]\n",
            "Validating..\n",
            "Validation loss:  0.1012611777654716\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9781526425745655\n",
            "\t f1 :  [0.81439178 0.99465131 0.83302975 0.92204751 0.88926843]\n",
            "\t average f1 :  0.8906777560903729\n",
            "\t confusion matrix :  [[ 1426   129    53    55   133]\n",
            " [   85 38959    36    41    19]\n",
            " [   40    55   686    12    32]\n",
            " [   36    42    10  1378    21]\n",
            " [  119    12    37    16  1562]]\n",
            "Epoch #11\n",
            "Training..\n",
            "Training loss:  0.010661895179914104\n",
            "Training metrics:\n",
            "\t accuracy :  0.9966720471891752\n",
            "\t f1 :  [0.973445   0.99943792 0.9772426  0.99735763 0.97779675]\n",
            "\t average f1 :  0.9850559802850715\n",
            "\t confusion matrix :  [[  9531     63     63     15    144]\n",
            " [    47 167141     14      2     13]\n",
            " [    41     31   4423      2     25]\n",
            " [    18      5      3  10946      8]\n",
            " [   129     13     27      5   8015]]\n",
            "Validating..\n",
            "Validation loss:  0.10679057772670474\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9770636084811308\n",
            "\t f1 :  [0.80525414 0.99449981 0.83150183 0.9179453  0.87704213]\n",
            "\t average f1 :  0.8852486432465387\n",
            "\t confusion matrix :  [[ 1410   136    47    64   139]\n",
            " [   76 38965    36    41    22]\n",
            " [   41    57   681    12    34]\n",
            " [   35    49     9  1376    18]\n",
            " [  144    14    40    18  1530]]\n",
            "Epoch #12\n",
            "Training..\n",
            "Training loss:  0.009684619700743092\n",
            "Training metrics:\n",
            "\t accuracy :  0.9970491312162972\n",
            "\t f1 :  [0.97698936 0.99953246 0.98066237 0.99798461 0.97833272]\n",
            "\t average f1 :  0.9867003035459092\n",
            "\t confusion matrix :  [[  9638     54     46     10    127]\n",
            " [    43 166753     11      1     13]\n",
            " [    32     19   4412      2     37]\n",
            " [    11      6      2  10894      6]\n",
            " [   131      9     25      6   7992]]\n",
            "Validating..\n",
            "Validation loss:  0.11138306771005903\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9763301773569809\n",
            "\t f1 :  [0.80884956 0.99390875 0.8177053  0.91149273 0.88036117]\n",
            "\t average f1 :  0.882463502375624\n",
            "\t confusion matrix :  [[ 1371   143    64    67   151]\n",
            " [   60 38916    68    66    30]\n",
            " [   25    54   702    12    32]\n",
            " [   30    41    11  1380    25]\n",
            " [  108    15    47    16  1560]]\n",
            "Epoch #13\n",
            "Training..\n",
            "Training loss:  0.015809154017242016\n",
            "Training metrics:\n",
            "\t accuracy :  0.9952479730336182\n",
            "\t f1 :  [0.97520745 0.998391   0.98300971 0.98204724 0.97999018]\n",
            "\t average f1 :  0.9837291153438941\n",
            "\t confusion matrix :  [[  9637     59     44     15    116]\n",
            " [    84 166605     17    330     11]\n",
            " [    24     18   4455     11     24]\n",
            " [    16      6      3  10913      8]\n",
            " [   132     12     13     10   7983]]\n",
            "Validating..\n",
            "Validation loss:  0.5334292522498539\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9279903987198294\n",
            "\t f1 :  [0.62206148 0.9690604  0.52054795 0.46897253 0.6180867 ]\n",
            "\t average f1 :  0.639745812233473\n",
            "\t confusion matrix :  [[ 1032   651    30     9    74]\n",
            " [   38 39073    18     3     8]\n",
            " [   29   421   361     3    11]\n",
            " [   43   967     6   461    10]\n",
            " [  380   389   147     3   827]]\n",
            "Epoch #14\n",
            "Training..\n",
            "Training loss:  0.04767330032255915\n",
            "Training metrics:\n",
            "\t accuracy :  0.9895463088051286\n",
            "\t f1 :  [0.94795444 0.99627933 0.93964075 0.97198176 0.95173176]\n",
            "\t average f1 :  0.9615176067890848\n",
            "\t confusion matrix :  [[  9280    271     78     26    200]\n",
            " [   128 166820     76     25     56]\n",
            " [    63    178   4211     10     65]\n",
            " [    35    400     21  10442     66]\n",
            " [   218    112     50     19   7749]]\n",
            "Validating..\n",
            "Validation loss:  0.0948830002120563\n",
            "Validation metrics:\n",
            "\t accuracy :  0.9800195581633107\n",
            "\t f1 :  [0.82098062 0.99540746 0.85085575 0.933423   0.89431361]\n",
            "\t average f1 :  0.8989960867951934\n",
            "\t confusion matrix :  [[ 1440   132    44    54   126]\n",
            " [   66 39014    34    16    10]\n",
            " [   41    51   696     9    28]\n",
            " [   40    38     6  1388    15]\n",
            " [  125    13    31    20  1557]]\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "## TODO: Re-run with unidirectional LSTMs\n",
        "## Keep other hyperparameters fixed\n",
        "train_val_loop_lstm({\n",
        "    \"bidirectional\": False,\n",
        "    \"batch_size\": 512,\n",
        "    \"d_emb\": 64,\n",
        "    \"d_hidden\": 128,\n",
        "    \"num_epochs\": 15,\n",
        "    \"learning_rate\": 0.005,\n",
        "    \"l2\": 1e-6,\n",
        "})\n",
        "## END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8UChDyKaPBs"
      },
      "source": [
        "### Questions **(2 points)**\n",
        "\n",
        "(a) How does the final performance of LSTMs compare to FFNNs? Is it better? What is a possible explanation?\n",
        "\n",
        "**TODO: Please fill in your answer here**\n",
        "\n",
        "**ans-a** :\n",
        "\n",
        "LSTMs are particularly well-suited for sequence data, such as natural language processing tasks like named entity recognition (NER), due to their ability to capture long-range dependencies and handle variable-length sequences. This is achieved through their internal gating mechanisms, which allow them to selectively remember or forget information over time. As a result, LSTMs are often more effective at capturing the sequential nature of the data and extracting meaningful patterns.\n",
        "\n",
        "On the other hand, FFNNs may struggle with sequential data because they treat each input independently and do not have mechanisms for modeling dependencies between inputs. They may also be limited by the fixed input size, which requires padding or truncation of sequences.\n",
        "(b) How does bidirectional LSTMs compare to unidirectional LSTMs? Why?\n",
        "\n",
        "**TODO: Please fill in your answer here**\n",
        "\n",
        "**ans-b **:\n",
        "\n",
        "Unidirectional LSTMs process input sequences in one direction, either from the beginning to the end (forward LSTM) or from the end to the beginning (backward LSTM).They are effective for tasks where the context leading up to the current position is more important for prediction.\n",
        "In natural language processing tasks like language modeling or sentiment analysis, where the meaning of a word often depends on preceding words, unidirectional LSTMs can capture this sequential context well.\n",
        "\n",
        "Bidirectional LSTMs process input sequences in both forward and backward directions simultaneously, merging the information from both directions.They are advantageous for tasks where context from both past and future positions is essential for accurate predictions.\n",
        "\n",
        "In tasks like named entity recognition (NER) or part-of-speech tagging, where understanding the context of a word requires information from both preceding and following words, bidirectional LSTMs can capture this bidirectional context effectively.\n",
        "By considering information from both directions, bidirectional LSTMs can potentially capture more comprehensive contextual information and make more informed predictions."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}